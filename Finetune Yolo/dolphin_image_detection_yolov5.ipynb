{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8623370,"sourceType":"datasetVersion","datasetId":5162402},{"sourceId":1418,"sourceType":"modelInstanceVersion","modelInstanceId":1199},{"sourceId":62513,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":52206}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Kaggle Notebook: People Detection in Security Camera Footage using YOLOv5 (GPU Support)\n\nimport torch\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Clone the YOLOv5 repository\n!git clone https://github.com/ultralytics/yolov5  # Cloning the repository\n%cd yolov5\n%pip install -r requirements.txt  # Installing requirements\n\nfrom models.common import DetectMultiBackend\nfrom utils.general import non_max_suppression\n\n# Check if GPU is available\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Using device: {device}')\n\n# Load YOLOv5 model\ndevice = torch.device(device)\nmodel = DetectMultiBackend('/kaggle/input/yolo-v5-dolphins/pytorch/v1/1/best.pt', device=device)\nmodel.eval()\n\n# Define a function to process video frames\ndef process_frame(frame, model):\n    # Preprocess the frame for YOLOv5\n    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    img = img / 255.0\n    img = torch.from_numpy(img).float().to(device).permute(2, 0, 1).unsqueeze(0)\n\n    # Run YOLOv5 on the frame\n    with torch.no_grad():\n        pred = model(img)\n\n    # Apply NMS\n    pred = non_max_suppression(pred)[0]\n    \n    # Extract bounding boxes and labels\n    boxes = pred.cpu().numpy()\n    labels = model.names\n    \n    return boxes, labels\n\n# Define a function to annotate frames with detected bounding boxes\ndef annotate_frame(frame, boxes, labels):\n    for box in boxes:\n        x1, y1, x2, y2, conf, cls = box\n        if labels[int(cls)] == 'dolphin':\n            # Draw bounding box\n            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n            # Add label\n            cv2.putText(frame, f'{labels[int(cls)]} {conf:.2f}', (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n    \n    return frame\n\n# Load the video file\nvideo_path = '/kaggle/input/exp-01-jun-2024-1145-cam1-4-mp4/Exp_01_Jun_2024_1145_cam1-4.mp4'\ncap = cv2.VideoCapture(video_path)\n\n# Get video properties\nfps = cap.get(cv2.CAP_PROP_FPS)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n# Define the codec and create VideoWriter object\noutput_path = '/kaggle/working/output_video.mp4'\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n# Check if video loaded successfully\nif not cap.isOpened():\n    print(\"Error opening video stream or file\")\n\n# Process the video frame by frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Split the frame into four quadrants\n    height, width, _ = frame.shape\n    half_height, half_width = height // 2, width // 2\n    \n    quadrants = [\n        frame[0:half_height, 0:half_width],\n        frame[0:half_height, half_width:width],\n        frame[half_height:height, 0:half_width],\n        frame[half_height:height, half_width:width]\n    ]\n    \n    # Process each quadrant\n    for i, quadrant in enumerate(quadrants):\n        # Ensure quadrant frame is in the correct format\n        quadrant_rgb = cv2.cvtColor(quadrant, cv2.COLOR_BGR2RGB)\n        boxes, labels = process_frame(quadrant_rgb, model)\n        quadrants[i] = annotate_frame(quadrant, boxes, labels)\n    \n    # Combine the quadrants back into a single frame\n    top_row = np.hstack((quadrants[0], quadrants[1]))\n    bottom_row = np.hstack((quadrants[2], quadrants[3]))\n    combined_frame = np.vstack((top_row, bottom_row))\n    \n    # Write the frame to the output video file\n    out.write(combined_frame)\n\n# Release video capture and writer objects\ncap.release()\nout.release()\n\nprint(\"Video processing complete. The output video is saved to /kaggle/working/output_video.mp4\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-07T18:03:13.834255Z","iopub.execute_input":"2024-06-07T18:03:13.835129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}